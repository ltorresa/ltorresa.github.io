<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="Generator" content="iWeb 3.0.4" />
    <meta name="iWeb-Build" content="local-build-20201003" />
    <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7" />
    <meta name="viewport" content="width=700" />
    <title>home</title>
    <link rel="stylesheet" type="text/css" media="screen,print" href="home_files/home.css" />
    <!--[if lt IE 8]><link rel='stylesheet' type='text/css' media='screen,print' href='home_files/homeIE.css'/><![endif]-->
    <!--[if gte IE 8]><link rel='stylesheet' type='text/css' media='screen,print' href='Media/IE8.css'/><![endif]-->
    <script type="text/javascript" src="Scripts/iWebSite.js"></script>
    <script type="text/javascript" src="Scripts/Widgets/SharedResources/WidgetCommon.js"></script>
    <script type="text/javascript" src="Scripts/Widgets/Navbar/navbar.js"></script>
    <script type="text/javascript" src="Scripts/iWebImage.js"></script>
    <script type="text/javascript" src="home_files/home.js"></script>
  </head>
  <body style="background: rgb(255, 255, 255); margin: 0pt; " onload="onPageLoad();" onunload="onPageUnload();">
    <div style="text-align: center; ">
      <div style="margin-bottom: 10px; margin-left: auto; margin-right: auto; margin-top: 10px; overflow: hidden; position: relative; word-wrap: break-word;  background: rgb(255, 255, 255); text-align: left; width: 700px; " id="body_content">
        <div style="margin-left: 0px; position: relative; width: 700px; z-index: 0; " id="nav_layer">
          <div style="height: 0px; line-height: 0px; " class="bumper"> </div>
          <div class="com-apple-iweb-widget-navbar flowDefining" id="widget0" style="margin-left: 20px; margin-top: 0px; opacity: 1.00; position: relative; width: 680px; z-index: 1; ">
    
            <div id="widget0-navbar" class="navbar">

      
              <div id="widget0-bg" class="navbar-bg">

        
                <ul id="widget0-navbar-list" class="navbar-list">
 <li></li> 
</ul>
                
      
</div>
              
    
</div>
          </div>
          <script type="text/javascript"><!--//--><![CDATA[//><!--
new NavBar('widget0', 'Scripts/Widgets/Navbar', 'Scripts/Widgets/SharedResources', '.', {"path-to-root": "", "navbar-css": ".navbar {\n\tfont-family: 'Helvetica Neue', Arial, sans-serif;\n\tfont-size: .8em;\n\tcolor: #666666;\n\tline-height: 30px;\n\tborder-bottom: 3px solid #ccc;\n}\n\n.navbar-bg {\n\ttext-align: right;}\n\n.navbar-bg ul {\n\tlist-style: none;\n\tmargin: 0px;\n\tpadding: 0px;\n}\n\n\nli {\n\tlist-style-type: none;\n\tdisplay: inline;\n\tpadding: 0px 5px 0px 0px;\n}\n\n\nli a {\n\ttext-decoration: none;\n\tpadding: 10px;\n\tcolor: #666666;\n\tfont-weight: bold;\n}\n\nli a:visited {\n\ttext-decoration: none;\n\tpadding: 10px;\n\tcolor: #666666;\n\tfont-weight: bold;\n}\n\nli a:hover\r{\r\n \tcolor: #999999;\n\ttext-decoration: none;\r}\n\n\nli.current-page a\r{\r\t color: #66ABC5;\n\ttext-decoration: none;\r}", "current-page-GUID": "53243ACB-6697-4184-B8D3-F5B11E08E9D2", "isCollectionPage": "NO"});
//--><!]]></script>
          <div style="clear: both; height: 0px; line-height: 0px; " class="spacer"> </div>
        </div>
        <div style="height: 75px; margin-left: 0px; position: relative; width: 700px; z-index: 10; " id="header_layer">
          <div style="height: 0px; line-height: 0px; " class="bumper"> </div>
        </div>
        <div style="margin-left: 0px; position: relative; width: 700px; z-index: 5; " id="body_layer">
          <div style="height: 0px; line-height: 0px; " class="bumper"> </div>
          <div style="height: 281px; width: 251px;  height: 281px; left: 34px; position: absolute; top: 22px; width: 251px; z-index: 1; " class="tinyText style_SkipStroke stroke_0">
            <img src="home_files/LT.jpg" alt="" style="border: none; height: 281px; width: 251px; " />
          </div>
          


          <div id="id1" style="height: 2476px; left: 41px; position: absolute; top: 342px; width: 613px; z-index: 1; " class="style_SkipStroke_1 shape-with-text">
            <div class="text-content style_External_613_2476" style="padding: 0px; ">
              <div class="style">
                <p style="padding-top: 0pt; " class="paragraph_style">recent news<br /></p>

                <p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span><b>I will join Northeastern University as a <a title="https://news.northeastern.edu/2022/10/27/alan-mckim-donation/" href="https://news.northeastern.edu/2022/10/27/alan-mckim-donation/"> Professor & President Joseph E. Aoun Chair</a> in Fall 2025.</b><br /></p></li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">                    
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span>I have several openings for postdocs, visiting researchers, and PhD students to work on exciting new projects in embodied AI, video understanding, and multimodal learning. Prospective applicants should contact me with a CV and a one-page research statement.
                  </li>
                  </ol> <br>

                <p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span>Our state-space video model <a title="https://sites.google.com/view/bimba-mllm" href="https://sites.google.com/view/bimba-mllm">BIMBA</a> won the <b>first place in the EgoSchema Challenge</b> at CVPR 2025<br /></p><br>                     
                  </ol>
		
                <p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span>Three of our recently published articles received <b><a title="https://egovis.github.io/awards/2023_2024//" href="https://egovis.github.io/awards/2023_2024/">Distinguished Paper Awards</a></b> at the CVPR 2025 EgoVis Workshop:<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://sites.google.com/view/vidrecap" href="https://sites.google.com/view/vidrecap">Video ReCap: Recursive Captioning of Hour-Long Videos</a>, <br />with Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, and Gedas Bertasius. Originally published at CVPR 2024.<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
		<p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7a65606fa1a6849450550325832036e5-Abstract-Datasets_and_Benchmarks.html" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7a65606fa1a6849450550325832036e5-Abstract-Datasets_and_Benchmarks.html">Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities</a>, <br />with Yale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang, and Miguel Martin. Originally published at NeurIPS 2023.<br /></p>
                  </li>                                    
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
		<p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://vision.cs.utexas.edu/projects/hiervl/">HierVL: Learning Hierarchical Video-Language Embeddings.</a>, <br />with Kumar Ashutosh, Rohit Girdhar, and Kristen Grauman. Originally published at CVPR 2023.<br /></p>
                  </li>                                    
<p  style="text-indent: 0px; " class="paragraph_style_2">These awards recognize publications that have advanced egocentric vision through original and innovative contributions.<br /></p><br>                     
                  </ol>
        
                    
                <p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span>Two papers to be presented at CVPR 2025:<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://sites.google.com/view/bimba-mllm" href="https://sites.google.com/view/bimba-mllm">BIMBA: Selective-Scan Compression for Long-Range Video Question Answering</a>, <br />with Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, and Gedas Bertasius.<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://arxiv.org/abs/2503.12855" href="https://arxiv.org/abs/2503.12855">VITED: Video Temporal Evidence Distillation</a>, <br />with Yujie Lu, Yale Song, William Wang, and Tushar Nagarajan.<br /></p>
                  </li>                  
                  </ol> <br>                                 


<p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span>One paper presented at ECCV 2024:<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://klauscc.github.io/4diff.html" href="https://klauscc.github.io/4diff.html">4Diff: 3D- Aware Diffusion Model for Third-to-First Viewpoint Translation</a>, <br />with Feng Cheng, Mi Luo, Huiyu Wang, Alex Dimakis, Gedas Bertasius, and Kristen Grauman.<br /></p>
</li>                  
                  </ol> <br>                            

              <p class="paragraph_style_1"><br /></p>
                <ol>
                  <li style="line-height: 20px; padding-left: 17px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_2"><span style="font-size: 15px; " class="Bullet">•</span><span style="width: 11px; " class="inline-block"></span>Four papers presented at CVPR 2024:<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://ego-exo4d-data.org/" href="https://ego-exo4d-data.org/">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</a>, <br />with 100 co-authors! Accepted as <b><u>oral (<1% accept rate)</u></b>.<br /></p>
                  </li>
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span>Learning to Segment Referred Objects from Narrated Egocentric Videos, <br />with Yuhan Shen, Huiyu Wang, Xitong Yang, Matt Feiszli, Ehsan Elhamifar, and Effrosyni Mavroudi. Accepted as <b><u>oral (<1% accept rate)</u></b>.<br /></p>
                  </li>                  
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span>Step Differences in Instructional Video, <br />with Tushar Nagarajan. <br /></p>
                  </li>                  
                  <li style="line-height: 20px; padding-left: 34px; text-indent: -17px; " class="full-width">
                    <p style="text-indent: -17px; " class="paragraph_style_3"><span style="font-family: '.SFNSText', '.SF NS Text'; font-size: 15px; font-stretch: normal; font-style: normal; font-weight: 400; " class="Bullet">✓</span><span style="width: 4px; " class="inline-block"></span><a title="https://sites.google.com/view/vidrecap" href="https://sites.google.com/view/vidrecap">Video ReCap: Recursive Captioning of Hour-Long Videos</a>, <br />with Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, and Gedas Bertasius. <br /></p>
                  </li>                  
                  </ol> <br>                                 
                                  
                <p class="paragraph_style_6"><br /></p>
                <p class="paragraph_style_7"><br /></p>
                <p class="paragraph_style">research<br /></p><br />
                <p class="paragraph_style_8">My research is in computer vision and multimodal (video-audio-language) learning. I aim to develop <em>perceptual AI agents</em> that can assist humans in their daily activities by understanding their behavior from video and communicating through language and actions. I am particularly motivated to apply this research to AI/AR coaching, episodic memory retrieval, and human-robot interaction.<br /></p>
              </div>
            </div>
          </div>
          


          <div id="id2" style="height: 302px; left: 314px; position: absolute; top: 7px; width: 377px; z-index: 1; " class="style_SkipStroke_1 shape-with-text">
            <div class="text-content style_External_377_302" style="padding: 0px; ">
              <div class="style">
                <p style="padding-top: 0pt; " class="paragraph_style_10">Lorenzo Torresani<br /></p>
                <p class="paragraph_style_11"><br /></p>
                <p style="padding-bottom: 0pt; " class="paragraph_style_11"><a title="mailto:torresani@gmail.com" href="mailto:torresani@gmail.com">Email</a>  / 
<a title="https://scholar.google.com/citations?user=ss8KR5gAAAAJ&hl=en&pagesize=1000&sortby=pubdate" href="https://scholar.google.com/citations?user=ss8KR5gAAAAJ&hl=en&pagesize=1000&sortby=pubdate">Google Scholar</a></p>

              </div>
            </div>
          </div>
          <div style="height: 1800px; line-height: 2318px; " class="spacer"> </div>
        </div>
        <div style="height: 75px; margin-left: 0px; position: relative; width: 700px; z-index: 15; " id="footer_layer">
          <div style="height: 0px; line-height: 0px; " class="bumper"> </div>
        </div>
      </div>
    </div>
  </body>
</html>


